## Using Computational Theory to Constrain Statistical Models of Neural Data

### Scott – June 22 2017

- New models $\to$ integrating them 
- **"Posterior" Behavioral Models** vs **"Prior" Computational Models**
  - Combine these two approaches
  - More data wont solve problems because it needs interpretation. 
  - But we also need falsifiability
  - **def** organon
- **Model Free**: all models have structure and structure $\approx$ assumption. 
  - We need to be vigilant about what assumptions we make and how they alter our interpretations. 
  - **eg:** Do the assumptions of PCA agree with structure of data. 

## Pipeline

- $\text{Data} \to \text{Model} \to \text{Consequences}$
- $\text{Model} \to \text{Estimation}$
- Summarized in Box's Loop (Box 1962, 1976, 1980)
- Computational Theory $\ne$ Statistical Model
  - The translation is very important; 
  - Must do better 
- **Box's Loop: ?The Scientific Method?**
  - Theory $\to\big($ Statistical Model $\leftarrow$ Data $\big)\to\big($ Inference $\to$ Analyze$\to$ Experimental Design $\to$ Data $\big) \to$ Criticise $\to$ Theory
  - Timescale is across research groups but also locally
- **Concerns**
  - *Computational learning theory*
  - Computationally Tractable Models $\cap$ Models We Believe In $\cap$ Experimentally Distinguishable Models
    - **eg**: Gibbs 
    - Runtime
    - Understandability and Interpretability
    - Experimentally Tractable/Testable

## Examples

- **Theory Driven Models**
  - biophysical Models, statistical translation 
  - lumped together, mean field type with biophysical constraints as warranted. More abstract in the data generating mechanism, not direct computational models
  - computational theories of population activity: Latent state $\mapsto$ Real state
    - diffusion spiking models for one dimensional latent space
  - cognitive theories and their constraints on the data we can collect. functional dependencies. $\mathcal{P}(\text{functions} \mid \text{data})$
- **Discrete Reusable States:** 
  - What I'm doing with Scott
  - Bayesian nonparametric prior on the number of states
  - neural network nonlinear mapping from discrete state to observed pixels
- **TD Worked Example. See paper**
  - revise model: poisson neurons, not gaussian neurons
  - critique can go to both computational model and the statistical model
  - *are mathematical computations limits on the kind of computations that can occur in nature*?
  - *modelling theory in a way that can interact with the data*
  - highly constrained parameters on the gaussian model.
  - now we can perform bayesian inference
  - integral is intractable: 
    - Exact inference
    - Monte Carlo Methods
    - Variational Inference: best approximating distributions
    - Probabalistic programming tools: but hides complexity
      - Edward
      - Stan
      - …
  - **Posterior Predictive Checks**
    - Generated data must look like the actual data
    - Choose what statistic you are trying to maximize based on what you know about the data (Berlin and Rubin, 1995; Blei 2014)
- **Experimental Design**
  - Try to pick an experiment to maximize insight
- [Link to code](http://github.com/slinderman/tdlds) 

## When Did this Pipeline Not Work

Bayesian Brain hypothesis is not easily falsifiable. Anything can work. 

## Analysis Must Reflect Experimental Design

How?

## New Analysis Must Build Upon Previous Stupid Knowledge

Make connections with old existing knowledge. You haven't built things up; you've gotten somewhere new when you just start by training a population level model, for example. 



